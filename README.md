<h1 align="center">
<img style="vertical-align:middle" width="400" height="180" src="https://raw.githubusercontent.com/wln20/Retrieval_QA/master/assets/logo.jpg" />
</h1>

<p align="center">
    <a href="https://github.com/wln20/Retrieval_QA">
        <img alt="develop" src="https://img.shields.io/badge/develop-v0.0-blue">
    </a>
    <a href="https://www.python.org/">
            <img alt="build" src="https://img.shields.io/badge/build-python-green">
    </a>
    <a href="https://github.com/wln20/Retrieval_QA/blob/master/LICENSE">
        <img alt="license" src="https://img.shields.io/badge/license-Apache_2.0-red">
    </a>
    <a href="https://github.com/wln20/Retrieval_QA/blob/master/raw_data">
        <img alt="download" src="https://img.shields.io/badge/download-raw-blue">
    </a>
      <a href="https://huggingface.co/datasets/lnwang/retrieval_qa">
        <img alt="huggingface" src="https://img.shields.io/badge/huggingface-dataset-yellow">
    </a>
</p>

NOTE: This project is still in the development stage, and many basic features are not yet available. We are actively developing it, please stay tuned. If you like this project, please give us a star to support it!

# Introduction
## About this repository
This repository contains the raw data of the dataset `Retrieval_QA`, along with example scripts to do evaluation on your customized models with `Retrieval_QA`. The dataset can be viewed and downloaded at: https://huggingface.co/datasets/lnwang/retrieval_qa


## About the dataset
### Basic information
The purpose of `Retrieval_QA` is to provide a simple and easy-to-use multilingual benchmark for retrieval encoders, which helps researchers quickly select the most effective retrieval encoder for text extraction and achieve optimal results in subsequent retrieval tasks such as retrieval-augmented-generation (RAG). The dataset contains multiple document-question pairs, where each document is a short text about the history, culture, or other information of a country or region, and each question is a query relevant to the content of the corresponding document.

Users may select a encoding model to encode each document and query into corresponding embeddings, and then use vector matching methods such as FAISS to identify the most relevant documents for each query as retrieval results. Then you may use the `acc ~ top-k` graph as a metric to evaluate whether this model act as a good encoder for retrieval.

Curated by <a href='https://wln20.github.io'>Luning Wang</a>

### Data source
The raw data was generated by GPT-3.5-turbo, using carefully designed prompts by human. The data was also cleaned to remove controversial information.

Now we support English(en), Simplified Chinese(zh_cn), Traditional Chinese(zh_tw), Japanese(ja), Spanish(es), German(de), Russian(ru),

### Metric
Here we use `acc ~ top-k` curve to show how good an encoder performs.

We first utilize an encoder to encode the raw docs and queries into embeddings. For each doc-query pair, it would be ideal that their embedding vectors are the nearest to each other, in terms of dot-similarity or cosine-similarity. However, the feature extraction ability of actual encoders may not be sufficient to ensure that the embeddings of each doc-query pair are the nearest. Therefore, we resort to the top-k strategy, which means that as long as the ground-truth doc is included in the top-k nearest docs in the retrieval result, we consider the retrieval "successful".

For each query, we use FAISS to find the top-k nearest docs and see if this retrieval is "successful". We perform this on all items in the dataset and calculate the proportion of successful retrievals in all the trials, which is denoted as `acc`. Take the following as an example: assume that we set `k=5` and it comes to `acc=0.8` after retrieval, then it means 80% queries are able to find their ground-truth doc in their top-5 retrieval results.

You can see that each `k` value would correspond to an `acc` value, so we would set different `k` values and calculate their corresponding `acc`, and we can draw an `acc ~ top-k` curve based on several specified `k` values. Here is an example graph, where the encoder is `bert-base-uncased` and the subset is `en`:
<img src='https://raw.githubusercontent.com/wln20/Retrieval_QA/master/results/retrieval_results/bert_en/eval_results.jpg'>
The blue line in the graph is `baseline`, calculated with the following formula:

$$
\mathrm{baseline}(n,k) = \frac{C_{n-1}^{k-1}}{C_n^k}
$$

$n$ is the number of all items,  and $\mathrm{baseline}(n,k)$ would be the probability of retrieving the correct doc after randomly sampling $k$ docs.

# Usage
## Environment setup
- Clone this repository:
    ```bash
    git clone https://github.com/wln20/Retrieval_QA.git
    cd Retrieval_QA
    ```
- Create a new conda environment and install the dependencies:
  ```bash
  conda create -n rqa python==3.10
  conda activate rqa
  pip install -r requirements.txt
  ```

## Load dataset
The dataset is available on ðŸ¤— Huggingface, you can conveniently use it in python with ðŸ¤— Datasets:
```python
from datasets import load_dataset
dataset_en = load_dataset('lnwang/retrieval_qa', name='en')
# dataset_zh_cn = load_dataset('lnwang/retrieval_qa', name='zh_cn')
# dataset_zh_tw = load_dataset('lnwang/retrieval_qa', name='zh_tw')
```
Now we support three languages: English(en), Simplified-Chinese(zh_cn), Traditional-Chinese(zh_tw). You can specify the `name` argument in `load_dataset()` to get the corresponding subset.

Note that we also provide the raw-data <a href='https://github.com/wln20/Retrieval_QA/tree/master/raw_data'>in this repository</a>, in `.jsonl` format, and the code of loading it has also been implemented in `get_embeddings.py`. But we recommend you to use `load_dataset()` as it's quite simple and convenient.  

## Basic usage
The pipeline could be decomposed into 2 steps:
- Step1: Use an encoder to encode the data(docs&queries) into embeddings, and cache them for retrieval later.
  ```bash
  python gen_embeddings.py --model [model] --subset [subset] --save_name [save_name]
  ```
- Step2: Use FAISS to do retrieval on the generated embeddings and get evaluation results.
  ```bash
  python retrieval_eval.py --name [name] [--verbose]
  ```
  
Take `bert-base-uncased` as an example encoder:
- Step1:
  ```bash
  python gen_embeddings.py --model bert-base-uncased --subset en --save_name bert
  ```
  The encoded embeddings would be saved into: `./embeddings/bert_en`
  ```bash
  .
    â”œâ”€â”€ doc_embeddings.npy
    â”œâ”€â”€ info.json
    â””â”€â”€ query_embeddings.npy

  ```
  `info.json` contains some information of the embeddings:
  ```json
  {"model": "bert-base-uncased", "embed_dim": 768, "subset": "en", "num_items": 196, "additional_info": ""}
  ```
- Step2:
  ```bash
  python retrieval_eval.py --name bert_en
  ```
  The result graph would be saved to: `./results/bert_en/eval_results.jpg`
  <img src='https://raw.githubusercontent.com/wln20/Retrieval_QA/master/results/retrieval_results/bert_en/eval_results.jpg'>
  You could also specify the `--verbose` argument to get a more detailed graph and a brief report:
  ```bash
  python retrieval_eval.py --name bert_en --verbose
  ```
  The result graph would be saved to: `./results/bert_en/eval_results_verbose.jpg`
  <img src='https://raw.githubusercontent.com/wln20/Retrieval_QA/master/results/retrieval_results/bert_en/eval_results_verbose.jpg'>
  The report would be saved to: `./results/bert_en/eval_report.md`
  ```
  # Evaluation report
  This report shows the top-k levels which are nearest to the checkpoints.
  + top_k=7 ~ acc=0.5, true_acc=0.541
  + top_k=37 ~ acc=0.8, true_acc=0.781
  + top_k=79 ~ acc=0.9, true_acc=0.898
  ```

More detailed instructions would be implemented in the future.
  
# Results
Some evaluation results are presented here:
- model: `bert-base-uncased`
  - subset: `en`
    <img src='https://raw.githubusercontent.com/wln20/Retrieval_QA/master/results/retrieval_results/bert_en/eval_results_verbose.jpg'>
  - subset: `zh_cn`
    <img src='https://raw.githubusercontent.com/wln20/Retrieval_QA/master/results/retrieval_results/bert_zh_cn/eval_results_verbose.jpg'>
  - subset: `zh_tw`
    <img src='https://raw.githubusercontent.com/wln20/Retrieval_QA/master/results/retrieval_results/bert_zh_tw/eval_results_verbose.jpg'>
  (Here we can see `bert-base-uncased` performs poorly on Chinese!)

- model: `baichuan-inc/Baichuan2-7B-Chat`
  - subset: `en`
    <img src='https://raw.githubusercontent.com/wln20/Retrieval_QA/master/results/retrieval_results/baichuan-7b_en/eval_results_verbose.jpg'>
  - subset: `zh_cn`
    <img src='https://raw.githubusercontent.com/wln20/Retrieval_QA/master/results/retrieval_results/baichuan-7b_zh_cn/eval_results_verbose.jpg'>
  - subset: `zh_tw`
    <img src='https://raw.githubusercontent.com/wln20/Retrieval_QA/master/results/retrieval_results/baichuan-7b_zh_tw/eval_results_verbose.jpg'>
    



# Trouble Shooting
1. If you're using baichuan model and run into this error: `AttributeError: 'BaichuanTokenizer' object has no attribute 'sp_model'`ï¼Œthe following solution may help: https://github.com/baichuan-inc/Baichuan2/issues/204#issuecomment-1756867868
